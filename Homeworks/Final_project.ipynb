{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00ffc03-ed4d-4da3-9ff1-acd214522830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "import corner\n",
    "import emcee\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#######TJ setting up the linear model######\n",
    "def linear_model(theta, x): #TJ define linear function in general terms\n",
    "    \"\"\"Simple y = mx+b model that returns a y value for a given m, b and x value\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    theta :  type = list - list of length 2 that contains a slope \"m\" value as the first entry and a y-intercept \"b\" value as its second entry\n",
    "    x : type = float - float representing an x value for which an associated y value will be returned\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    a single y-value\n",
    "    \"\"\"\n",
    "    m, b = theta\n",
    "    return m * x + b\n",
    "\n",
    "def log_prior_linear(theta): #TJ assign priors to eliminate impossible values that are definitely not correct\n",
    "    \"\"\"function to assign uniform prior weights to a bayesian analysis\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    theta :  type = list - list of length 2 that contains a slope \"m\" value as the first entry and a y-intercept \"b\" value as its second entry\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    either zero or negative infinity depending on whether or not the passed m and b values are physically realistic to even consider.      \n",
    "    \"\"\"\n",
    "    m, b = theta\n",
    "    if -5 < m < 0 and -7 < b < 7:\n",
    "        return 0.0  # log(1)\n",
    "    return -np.inf\n",
    "\n",
    "def log_likelihood_linear(theta, x, y, yerr): #TJ define log likelihood function for the linear fit\n",
    "    \"\"\"Uses the formula for log of the likelihood of the linear model\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    theta :  type = list - list of length 2 that contains a slope \"m\" value as the first entry and a y-intercept \"b\" value as its second entry\n",
    "    x : type = list of floats - float representing an x value for an associated y value\n",
    "    y : type = list of floats - float representing an y value for an associated x value\n",
    "    yerr : type = list of floats - float representing the error on a given y value\n",
    "    Returns\n",
    "    -------------\n",
    "    float to be compared to other outputs for different m,b values, closer to zero is better.   \n",
    "    \"\"\"\n",
    "    model = linear_model(theta, x)\n",
    "    return -0.5 * np.sum(((y - model) / yerr) ** 2 + np.log(2 * np.pi * yerr**2))\n",
    "\n",
    "def log_posterior_linear(theta, x, y, yerr): #TJ define a function to add the priors to the log likelihood\n",
    "    \"\"\"scales the log of the likelihood by the priors assigned in the log_prior_linear function\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    theta :  type = list - list of length 2 that contains a slope \"m\" value as the first entry and a y-intercept \"b\" value as its second entry\n",
    "    x : type = float - float representing an x value for which an associated y value will be returned\n",
    "    y : type = list of floats - float representing an y value for an associated x value\n",
    "    yerr : type = list of floats - float representing the error on a given y value\n",
    "    Returns\n",
    "    -------------\n",
    "    float to be compared to other outputs for different m,b values, closer to zero is better.         \n",
    "    \"\"\"\n",
    "    lp = log_prior_linear(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood_linear(theta, x, y, yerr)\n",
    "\n",
    "\n",
    "#####TJ setting up the quadratic model######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def quadratic_model(theta, x): #TJ define linear function in general terms\n",
    "    \"\"\"Simple y = a2*x^2 + a1*x + a0 model that returns a y value for a given x value based on a2, a1, a0\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    theta :  type = list - list of length 3 that contains entries in order [a2, a1, a0]\n",
    "    x : type = float - float representing an x value for which an associated y value will be returned\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    a single y-value        \n",
    "    \"\"\"\n",
    "    a2, a1, a0 = theta\n",
    "    return a2 * x**2 + a1 * x + a0\n",
    "\n",
    "def log_prior_quadratic(theta): #TJ assign priors to eliminate impossible values that are definitely not correct\n",
    "    \"\"\"function to assign uniform prior weights to a bayesian analysis\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    theta :  type = list - list of length 3 that contains entries in order [a2, a1, a0]\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    either zero or negative infinity depending on whether or not the passed a2, a1, a0 values are physically realistic to even consider.      \n",
    "    \"\"\"\n",
    "    a2, a1, a0 = theta\n",
    "    if -2 < a2 < 2 and -5 < a1 < 5 and -7 < a0 < 7:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def log_likelihood_quadratic(theta, x, y, yerr): #TJ define log likelihood function for the linear fit\n",
    "    \"\"\"Uses the formula for log of the likelihood of the quadratic model, I could have put both models under the same function\n",
    "        and then just left which model it should use to be determined by an argument, but I have already copied and pasted it...\n",
    "        ah... hindsight is 20/20 I guess\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    theta :  type = list - list of length 3 that contains entries in order [a2, a1, a0]\n",
    "    x : type = list of floats - float representing an x value for an associated y value\n",
    "    y : type = list of floats - float representing an y value for an associated x value\n",
    "    yerr : type = list of floats - float representing the error on a given y value\n",
    "    Returns\n",
    "    -------------\n",
    "    float to be compared to other outputs for different coefficient values, closer to zero is better.   \n",
    "    \"\"\"\n",
    "    model = quadratic_model(theta, x)\n",
    "    return -0.5 * np.sum(((y - model) / yerr) ** 2 + np.log(2 * np.pi * yerr**2))\n",
    "\n",
    "def log_posterior_quadratic(theta, x, y, yerr): #TJ define a function to add the priors to the log likelihood\n",
    "    \"\"\"scales the log of the likelihood by the priors assigned in the log_prior_quadratic function\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    theta :  type = list - list of length 3 that contains entries in order [a2, a1, a0]\n",
    "    x : type = float - float representing an x value for which an associated y value will be returned\n",
    "    y : type = list of floats - float representing an y value for an associated x value\n",
    "    yerr : type = list of floats - float representing the error on a given y value\n",
    "    Returns\n",
    "    -------------\n",
    "    float to be compared to other outputs for different coefficient values, closer to zero is better.         \n",
    "    \"\"\"\n",
    "    lp = log_prior_quadratic(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood_quadratic(theta, x, y, yerr)\n",
    "\n",
    "##########TJ run the model fit procedure from emcee############\n",
    "\n",
    "\n",
    "def run_emcee(log_posterior, ndim, nwalkers=32, nsteps=5000, initial_guess=None):\n",
    "    \"\"\"runs the emcee protocol for a set of data\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    log_posterior :  type = function - function determining the log_posterior for a given model (including priors)\n",
    "    ndim : type = int - integer representing the number of parameters that the model needs to fit\n",
    "    nwalkers (optional, defaults to 32) : type = int - integer number of walkers used when sampling the parameter space (should be at least twice ndim)\n",
    "    nsteps (optional, defaults to 5000) : type = int - integer number of steps each walker will take on its path\n",
    "    initial_guess (optional, defaults to none) : type = float - initial starting points for the walkers, will be randomly generated if left as none\n",
    "    Returns\n",
    "    -------------\n",
    "    float to be compared to other outputs for different coefficient values, closer to zero is better.         \n",
    "    \"\"\"\n",
    "    \n",
    "    if initial_guess is None:\n",
    "        initial_guess = np.random.randn(nwalkers, ndim) #TJ create starting points for walkers\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=(x, y, yerr)) #TJ initialize the samplers\n",
    "    sampler.run_mcmc(initial_guess, nsteps, progress=True) #TJ perform the emcee protocol\n",
    "    return sampler\n",
    "\n",
    "def summarize_samples(samples, labels):\n",
    "    \"\"\"summarizes the samples statistics to produce the 16, 50, 84 precentile values\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    samples :  type = np array - array containing \n",
    "    ndim : type = int - integer representing the number of parameters that the model needs to fit\n",
    "    nwalkers (optional, defaults to 32) : type = int - integer number of walkers used when sampling the parameter space (should be at least twice ndim)\n",
    "    nsteps (optional, defaults to 5000) : type = int - integer number of steps each walker will take on its path\n",
    "    initial_guess (optional, defaults to none) : type = float - initial starting points for the walkers, will be randomly generated if left as none\n",
    "    Returns\n",
    "    -------------\n",
    "    float to be compared to other outputs for different coefficient values, closer to zero is better.         \n",
    "    \"\"\"\n",
    "    \n",
    "    percentiles = np.percentile(samples, [16, 50, 84], axis=0) #TJ calculate the 16th, 50th and 84 percentile values\n",
    "    summaries = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        p16, p50, p84 = percentiles[:, i]\n",
    "        summaries[label] = (p50, p50 - p16, p84 - p50)\n",
    "    return summaries\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    import argparse\n",
    "    #TJ add argparse object and description\n",
    "    parser = argparse.ArgumentParser(description=\"\"\"Runs emcee package on data stored at /d/scratch/ASTR5160/final/dataxy.fits to fit both\n",
    "    a linear model (m and b values) as well as a quadratic (a2, a1, a0 values). Then using 16, 50, and 84 percentile values, reports the best\n",
    "    fit parameters as median plus or minus 1 sigma. Then reports on whether the linear fit is sufficient to model the data (it is not).\n",
    "    \n",
    "    \"\"\") \n",
    "\n",
    "\n",
    "    \n",
    "    data_file_path = '/d/scratch/ASTR5160/final/dataxy.fits' #TJ assign datafile location\n",
    "    data = Table.read(data_file_path)\n",
    "    x = data['x']\n",
    "    y = data['y']\n",
    "    yerr = data['yerr']\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #TJ run the linear fit analysis\n",
    "    sampler_linear = run_emcee(log_posterior_linear, ndim=2)\n",
    "    samples_linear = sampler_linear.get_chain(discard=1000, thin=15, flat=True) #TJ discard the first 1000 steps to ensure convergence\n",
    "    \n",
    "    # Run quadratic fit\n",
    "    sampler_quadratic = run_emcee(log_posterior_quadratic, ndim=3)\n",
    "    samples_quadratic = sampler_quadratic.get_chain(discard=1000, thin=15, flat=True)\n",
    "    lin_labels = [\"m\", \"b\"]\n",
    "    summary_linear = summarize_samples(samples_linear, lin_labels )\n",
    "    quad_labels = [\"a2\", \"a1\", \"a0\"]\n",
    "    summary_quadratic = summarize_samples(samples_quadratic, quad_labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    xfit = np.linspace(min(x), max(x), 1000) #TJ make a higher resolution x-axis for fitted functions\n",
    "    \n",
    "    # Plot linear fit\n",
    "    plt.errorbar(x, y, yerr=yerr, fmt='.k', label='Data')\n",
    "    plt.plot(xfit, linear_model([summary_linear['m'][0], summary_linear['b'][0]], xfit), color = 'red', label = 'best fit linear')\n",
    "    plt.plot(xfit, quadratic_model([summary_quadratic['a2'][0], summary_quadratic['a1'][0], summary_quadratic['a0'][0]], xfit), color = 'blue', label = 'best fit quadratic')\n",
    "    \n",
    "    plt.title('fitted functions compared to real data')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    fig = corner.corner(samples_linear, labels=lin_labels,\n",
    "                        truths=[summary_linear[label][0] for label in lin_labels])\n",
    "    fig.suptitle(\"Posterior Distributions for Linear Model\")\n",
    "    plt.show()\n",
    "    \n",
    "    fig = corner.corner(samples_quadratic, labels=quad_labels,\n",
    "                        truths=[summary_quadratic[label][0] for label in quad_labels])\n",
    "    fig.suptitle(\"Posterior Distributions for Quadratic Model\")\n",
    "    plt.show()\n",
    "    #TJ this commented out section displays the values and their +/- 1sigma limits very neatly, but this does not work well from the command line\n",
    "    \n",
    "    print(\"fitted m and b values based on 16%, 50%, and 84% uncertainties :\")\n",
    "    labels = [\"m\", \"b\"]\n",
    "    for i in range(samples_linear.shape[1]):\n",
    "        mcmc = np.percentile(samples_linear[:, i], [16, 50, 84])\n",
    "        lower = mcmc[1] - mcmc[0]\n",
    "        upper = mcmc[2] - mcmc[1]\n",
    "        print(f'{labels[i]} : {mcmc[1]} (+{upper} / - {lower})')\n",
    "    print() #TJ blank line for clarity\n",
    "    print(\"fitted a2, a1, and a0 values based on 16%, 50%, and 84% uncertainties :\")\n",
    "    labels = [\"a2\", \"a1\", \"a0\"]\n",
    "    for i in range(samples_quadratic.shape[1]):\n",
    "        mcmc = np.percentile(samples_quadratic[:, i], [16, 50, 84])\n",
    "        lower = mcmc[1] - mcmc[0]\n",
    "        upper = mcmc[2] - mcmc[1]\n",
    "        print(f'{labels[i]} : {mcmc[1]} (+{upper} / - {lower})')\n",
    "    print() #TJ leave blank space for clarity\n",
    "    print('Based on the information from the probability distribution, it is unlikely that the linear model will be sufficient to fit the data.')\n",
    "    print('If the a2 values had +/- 1sigma values that straddled zero, then I would feel ok just using a linear fit model, but as it stands,')\n",
    "    print('the a2 values are NOT consistent with zero, and therefore we do actually need a quadratic fit.')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
